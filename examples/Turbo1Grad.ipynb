{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example of TuRBO-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turbo.turbo_1_grad import Turbo1Grad\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up an optimization problem class\n",
    "\n",
    "In this TuRBO with derivative example, we'll be setting up a function as follows.\n",
    "\n",
    "First, we'll create a true linear function in 3 dimensions with weights $w^{*} = [1.1, 0.8, 0.1]$ and a bias of $b^{*} = 2.0$. Then we'll sample 1000 data points in 3 dimensions, and set the labels to be $y_{i} = w^{*\\top}x_{i} + b^{*} + \\epsilon_{i}$, where $\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}_{n})$. We set $\\sigma^{2}_{n}$ such that there is roughly a 10:1 signal to noise ratio in the data.\n",
    "\n",
    "Given `data_x` and `data_y`, the function we'll seek to optimize will be to fit a linear model to this data, not assuming we know the optimal parameters $w^{*},b^{*}$. In other words, our \"blackbox\" objective function with gradients will be $$f(w, b)=\\frac{1}{N}\\sum_{i=1}^{N}\\left(y_{i} - [w^{\\top}x_{i} + b]\\right)^{2}$$.\n",
    "\n",
    "Obviously, this function should have a global optimum very close to $w^{*}, b^{*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create data_x and data_y as described above, with true_weights being w* and true_bias being b*\n",
    "\n",
    "true_weights = torch.tensor([1.1, 0.8, 0.1], dtype=torch.float32).view(3)\n",
    "true_bias = 2.0\n",
    "\n",
    "data_x = torch.randn(1000, 3)\n",
    "data_y = data_x.matmul(true_weights)\n",
    "data_y = data_y + torch.randn_like(data_y) * (0.1 * data_y.std()) + true_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a simple linear model in PyTorch, where forward(x) returns w'x + b.\n",
    "    \n",
    "    The weights and bias parameters are learnable, which will let us get derivatives for them easily.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_dims):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.zeros(num_dims))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.matmul(self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline / Sanity Check\n",
    "\n",
    "In the next cell, we do a simple baseline / sanity check where we train the linear model with gradient descent (Adam). Obviously, since our function is convex in this case, there should be no trouble recovering nearly the exactly correct weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Loss = 5.91\n",
      "Iteration 50 - Loss = 2.85\n",
      "Iteration 100 - Loss = 1.32\n",
      "Iteration 150 - Loss = 0.60\n",
      "Iteration 200 - Loss = 0.26\n",
      "Iteration 250 - Loss = 0.11\n",
      "Iteration 300 - Loss = 0.05\n",
      "Iteration 350 - Loss = 0.03\n",
      "Iteration 400 - Loss = 0.02\n",
      "Iteration 450 - Loss = 0.02\n",
      "Learned weights: tensor([1.0978, 0.7969, 0.0988]) - Learned bias: 1.98\n",
      "True weights: tensor([1.1000, 0.8000, 0.1000]) - True bias: 2.0\n"
     ]
    }
   ],
   "source": [
    "lm = LinearModel(3)\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(lm.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(500):\n",
    "    lm.zero_grad()\n",
    "    output = lm(data_x)\n",
    "    loss = (data_y - output).pow(2).mean()\n",
    "    loss.backward()\n",
    "    if i % 50 == 0:\n",
    "        print(f'Iteration {i} - Loss = {loss.item():.2f}')\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'Learned weights: {lm.weights.data} - Learned bias: {lm.bias.data.item():.2f}')\n",
    "print(f'True weights: {true_weights} - True bias: {2.0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define blackbox objective class\n",
    "\n",
    "Next, we define a class similar to what is normally used for TuRBO. \n",
    "\n",
    "The `__call__` method here instantiates a `LinearModel` object using the class above, but fills the weights with the first three entries in `x` and fills the bias with the last entry in `x`. Then, we compute the loss as normal, and compute the derivatives with respect to the weights and the bias, which we pull out in to a single `grad` vector.\n",
    "\n",
    "The return of this call is a `dim + 1` vector of the form `[f(x), [df/dx]_1, [df/dx]_2, [df/dx]_3, [df/dx]_4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModelFunction(object):\n",
    "    def __init__(self, data_x, data_y, dim=4):\n",
    "        self.dim = dim\n",
    "        self.lb = -2 * np.ones(dim)\n",
    "        self.ub = 2 * np.ones(dim)\n",
    "        \n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = torch.from_numpy(x)\n",
    "        lb_ = torch.from_numpy(self.lb)\n",
    "        ub_ = torch.from_numpy(self.ub)\n",
    "        assert len(x) == self.dim\n",
    "        assert torch.all(x >= lb_)\n",
    "        assert torch.all(x <= ub_)\n",
    "        \n",
    "        lm = LinearModel(num_dims=self.dim - 1)\n",
    "        lm.weights.data.copy_(x[:3])\n",
    "        lm.bias.data.copy_(x[-1])\n",
    "        \n",
    "        output = lm(data_x)\n",
    "        loss = (data_y - output).pow(2).mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        grad = torch.cat([lm.weights.grad, lm.bias.grad])\n",
    "        return np.hstack([loss.item(), grad.cpu().detach().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmf = LinearModelFunction(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02016393, 0.00554729, 0.00578184, 0.00085408, 0.01044734])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmf(np.array([1.1, 0.8, 0.1, 2.0]))  # Optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Turbo optimizer instance\n",
    "\n",
    "Everything from this point on is identical to the standard TuRBO setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype = torch.float64 \n",
      "Using device = cpu\n"
     ]
    }
   ],
   "source": [
    "turbo1 = Turbo1Grad(\n",
    "    f=lmf,  # Handle to objective function\n",
    "    lb=lmf.lb,  # Numpy array specifying lower bounds\n",
    "    ub=lmf.ub,  # Numpy array specifying upper bounds\n",
    "    n_init=20,  # Number of initial bounds from an Latin hypercube design\n",
    "    max_evals = 1000,  # Maximum number of evaluations\n",
    "    batch_size=10,  # How large batch size TuRBO uses\n",
    "    verbose=True,  # Print information from each batch\n",
    "    use_ard=True,  # Set to true if you want to use ARD for the GP kernel\n",
    "    max_cholesky_size=2000,  # When we switch from Cholesky to Lanczos\n",
    "    n_training_steps=50,  # Number of steps of ADAM to learn the hypers\n",
    "    min_cuda=1024,  # Run on the CPU for small datasets\n",
    "    device=\"cpu\",  # \"cpu\" or \"cuda\"\n",
    "    dtype=\"float64\",  # float64 or float32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from fbest = 1.963\n",
      "30) New best: 1.244\n",
      "40) New best: 0.8119\n",
      "50) New best: 0.3613\n",
      "60) New best: 0.2452\n",
      "70) New best: 0.1934\n",
      "80) New best: 0.1626\n",
      "90) New best: 0.1485\n",
      "90) Restarting with fbest = 0.1485\n",
      "Starting from fbest = 1.869\n",
      "150) New best: 0.1458\n",
      "160) New best: 0.1033\n",
      "170) New best: 0.07771\n",
      "180) New best: 0.06606\n",
      "180) Restarting with fbest = 0.06606\n",
      "Starting from fbest = 1.444\n",
      "270) Restarting with fbest = 0.141\n",
      "Starting from fbest = 1.024\n",
      "320) New best: 0.03518\n",
      "340) New best: 0.02846\n",
      "350) New best: 0.02442\n",
      "360) New best: 0.02178\n",
      "360) Restarting with fbest = 0.02178\n",
      "Starting from fbest = 2.293\n",
      "450) Restarting with fbest = 0.02311\n",
      "Starting from fbest = 2.933\n",
      "540) Restarting with fbest = 0.02561\n",
      "Starting from fbest = 2.021\n",
      "630) Restarting with fbest = 0.1422\n",
      "Starting from fbest = 0.4744\n",
      "720) Restarting with fbest = 0.02496\n",
      "Starting from fbest = 1.621\n",
      "810) Restarting with fbest = 0.1009\n",
      "Starting from fbest = 1.143\n",
      "900) Restarting with fbest = 0.08303\n",
      "Starting from fbest = 2.391\n",
      "990) New best: 0.02089\n",
      "990) Restarting with fbest = 0.02089\n",
      "Starting from fbest = 1.925\n"
     ]
    }
   ],
   "source": [
    "turbo1.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract all evaluations from Turbo and print the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = turbo1.X  # Evaluated points\n",
    "fX = turbo1.fX  # Observed values\n",
    "ind_best = np.argmin(fX[:, 0])  # The first column is the actual function value, so argmin over that.\n",
    "f_best, x_best = fX[ind_best], X[ind_best, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value (and gradient) was tensor([ 0.0209, -0.0322,  0.0264,  0.0200, -0.0318], dtype=torch.float64)\n",
      "Best weights were tensor([1.0807, 0.8103, 0.1086], dtype=torch.float64) and the bias was 1.98\n"
     ]
    }
   ],
   "source": [
    "print(f'Best value (and gradient) was {torch.from_numpy(f_best)}')\n",
    "print(f'Best weights were {torch.from_numpy(x_best[:3])} and the bias was {x_best[-1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
